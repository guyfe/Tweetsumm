{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"665310cd876a49c8a14e90cc4fe3b9a4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb249a5c925b4070afb3571f43158fd0","IPY_MODEL_e09539267ff04ea0b92b95ee7b779973","IPY_MODEL_15ce900f4e914f6d8016c2e0089591e9"],"layout":"IPY_MODEL_c68e86996b3445aab1d41326b8db8581"}},"eb249a5c925b4070afb3571f43158fd0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f14fc61a79c0434f8ed70fb50f92cc5a","placeholder":"​","style":"IPY_MODEL_3bb7b69021a741c3a4f9850982e34219","value":"config.json: 100%"}},"e09539267ff04ea0b92b95ee7b779973":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e7c908a98b947778229075ec157a83d","max":664,"min":0,"orientation":"horizontal","style":"IPY_MODEL_964dbb4570034e2a858c5fab6686da2b","value":664}},"15ce900f4e914f6d8016c2e0089591e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ceff80939d245a9ac16a6b5d799d1d4","placeholder":"​","style":"IPY_MODEL_4af14e169ece4f67b722e7d5cc7573e3","value":" 664/664 [00:00&lt;00:00, 16.5kB/s]"}},"c68e86996b3445aab1d41326b8db8581":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f14fc61a79c0434f8ed70fb50f92cc5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bb7b69021a741c3a4f9850982e34219":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e7c908a98b947778229075ec157a83d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"964dbb4570034e2a858c5fab6686da2b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6ceff80939d245a9ac16a6b5d799d1d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4af14e169ece4f67b722e7d5cc7573e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70716fd6a8614cdd9247d151c9e91066":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1cdc99e85bc644449b2addb767e79d45","IPY_MODEL_da6d26dfea624a1e91dc0e096f348ec5","IPY_MODEL_3db5cc497fff4907972b7a29d2cb190e"],"layout":"IPY_MODEL_a30d3b2c75624812830039d4b0aef42e"}},"1cdc99e85bc644449b2addb767e79d45":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc4f35404f254513b73eabe7f9d316c6","placeholder":"​","style":"IPY_MODEL_a8bef80bff3a4df1b78731842a25ec7e","value":"model.safetensors: 100%"}},"da6d26dfea624a1e91dc0e096f348ec5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_861c47691d41450f8ca6e39bf583f670","max":438080896,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d968dddce6e1440db612349f99f71441","value":438080896}},"3db5cc497fff4907972b7a29d2cb190e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c56c6042d4af490a92829c0e5e125300","placeholder":"​","style":"IPY_MODEL_a581fbf236d9417eac2ce2b77a6505fd","value":" 438M/438M [00:10&lt;00:00, 31.6MB/s]"}},"a30d3b2c75624812830039d4b0aef42e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc4f35404f254513b73eabe7f9d316c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8bef80bff3a4df1b78731842a25ec7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"861c47691d41450f8ca6e39bf583f670":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d968dddce6e1440db612349f99f71441":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c56c6042d4af490a92829c0e5e125300":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a581fbf236d9417eac2ce2b77a6505fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7256212,"sourceType":"datasetVersion","datasetId":4204816}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets evaluate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sor2QwG4Dq0S","outputId":"42c720b2-c6b5-4279-8e8b-ede753e4c16f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[torch]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2SAIdERuFfBB","outputId":"3abbca17-f377-4f25-814e-e173b2d06850","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom ast import literal_eval\nimport re\nimport string","metadata":{"id":"YTqFHvX6Fi4o","execution":{"iopub.status.busy":"2023-12-21T16:44:30.498340Z","iopub.execute_input":"2023-12-21T16:44:30.498734Z","iopub.status.idle":"2023-12-21T16:44:30.945343Z","shell.execute_reply.started":"2023-12-21T16:44:30.498697Z","shell.execute_reply":"2023-12-21T16:44:30.944465Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = pd.read_excel('/kaggle/input/dialogs/dialogs_last.xlsx')","metadata":{"id":"GsicZFRmFlp_","execution":{"iopub.status.busy":"2023-12-21T16:44:30.948078Z","iopub.execute_input":"2023-12-21T16:44:30.948549Z","iopub.status.idle":"2023-12-21T16:44:31.649293Z","shell.execute_reply.started":"2023-12-21T16:44:30.948519Z","shell.execute_reply":"2023-12-21T16:44:31.648258Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#label_name - название основной метки, replics_column_name - доп метки с разметкой по репликам\nlabel_name = 'is_negative'\nreplics_column_name = 'negative_replics'","metadata":{"id":"PTcmuYJFYbMH","execution":{"iopub.status.busy":"2023-12-21T16:44:31.650776Z","iopub.execute_input":"2023-12-21T16:44:31.651248Z","iopub.status.idle":"2023-12-21T16:44:31.656077Z","shell.execute_reply.started":"2023-12-21T16:44:31.651214Z","shell.execute_reply":"2023-12-21T16:44:31.654943Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df[label_name].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9CmmkAtFFmq9","outputId":"8de27e58-a87a-4e49-86f4-4716a2391d2f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df[label_name].isin([0, 1])].reset_index(drop=True)","metadata":{"id":"VX_dv_h1FoY9","execution":{"iopub.status.busy":"2023-12-21T16:44:31.688002Z","iopub.execute_input":"2023-12-21T16:44:31.688683Z","iopub.status.idle":"2023-12-21T16:44:31.709548Z","shell.execute_reply.started":"2023-12-21T16:44:31.688650Z","shell.execute_reply":"2023-12-21T16:44:31.708614Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df[label_name] = df[label_name].astype('int')","metadata":{"id":"hfw2rRT20Oir","execution":{"iopub.status.busy":"2023-12-21T16:44:31.710760Z","iopub.execute_input":"2023-12-21T16:44:31.711135Z","iopub.status.idle":"2023-12-21T16:44:31.719365Z","shell.execute_reply.started":"2023-12-21T16:44:31.711105Z","shell.execute_reply":"2023-12-21T16:44:31.718376Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Разбить текст диалога на реплики клиента","metadata":{"id":"M3H_t_J-CMeC"}},{"cell_type":"code","source":"def get_client_replics_list(dialog_text):\n    replics = dialog_text.replace('Customer:', '').split('\\n\\t')\n    replics = [replic for replic in replics if not 'Agent:' in replic]\n    replics = [replic.strip() for replic in replics]\n    return replics","metadata":{"id":"i9glhoOPCQWO","execution":{"iopub.status.busy":"2023-12-21T16:44:32.368782Z","iopub.execute_input":"2023-12-21T16:44:32.369069Z","iopub.status.idle":"2023-12-21T16:44:32.377073Z","shell.execute_reply.started":"2023-12-21T16:44:32.369044Z","shell.execute_reply":"2023-12-21T16:44:32.376092Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def get_label_replics_list(label_replics_str):\n    if type(label_replics_str) != str:\n        return []\n    label_replics_list = [replic.strip() for replic in label_replics_str.split('\\n\\t;')]\n    return label_replics_list","metadata":{"id":"c5cWKbSY33CQ","execution":{"iopub.status.busy":"2023-12-21T16:44:32.378240Z","iopub.execute_input":"2023-12-21T16:44:32.378621Z","iopub.status.idle":"2023-12-21T16:44:32.385687Z","shell.execute_reply.started":"2023-12-21T16:44:32.378590Z","shell.execute_reply":"2023-12-21T16:44:32.384755Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def make_replics_df(df, replics_column_name, label_name):\n    replics_df = df[['text', replics_column_name]]\n    replics_df[replics_column_name] = replics_df[replics_column_name].\\\n        map(get_label_replics_list)\n    replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n    neg_replics = replics_df[replics_column_name].reset_index().explode(replics_column_name).dropna()\n    replics_df = replics_df.explode('text').reset_index().rename({'index': 'dialog_id'}, axis=1)\n    replics_df[label_name] = replics_df.apply(lambda x: x['text'] in x[replics_column_name], axis=1)\n    neg_replics['is_bad_replic'] = neg_replics[replics_column_name].apply(lambda x: x not in replics_df['text'].values and x != '')\n    bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n    replics_df = replics_df[~replics_df.index.isin(bad_indices)][['dialog_id', 'text', label_name]].reset_index(drop=True)\n    replics_df[label_name] = replics_df[label_name].astype('int')\n    return replics_df","metadata":{"id":"kPk7mozQg2xN","execution":{"iopub.status.busy":"2023-12-21T16:44:32.386835Z","iopub.execute_input":"2023-12-21T16:44:32.387110Z","iopub.status.idle":"2023-12-21T16:44:32.400734Z","shell.execute_reply.started":"2023-12-21T16:44:32.387087Z","shell.execute_reply":"2023-12-21T16:44:32.399717Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Предобработка","metadata":{"id":"YNRnCvU1kiUj"}},{"cell_type":"code","source":"def remove_html_tags(text):\n    clean = re.compile('<.*?>')\n    return re.sub(clean, '', text)\n\ndef remove_nicknames(text):\n    clean = re.compile('@\\w+')\n    return re.sub(clean, '', text)\n\ndef remove_urls(text):\n    clean = re.compile(r'http\\S+|www\\S+')\n    return re.sub(clean, '', text)\n\ndef clean_text(text):\n    clean = re.compile(r'[-,:]')\n    return re.sub(clean, '', text)","metadata":{"id":"HQ3zApvqYaLi","execution":{"iopub.status.busy":"2023-12-21T16:44:32.685490Z","iopub.execute_input":"2023-12-21T16:44:32.687187Z","iopub.status.idle":"2023-12-21T16:44:32.693682Z","shell.execute_reply.started":"2023-12-21T16:44:32.687156Z","shell.execute_reply":"2023-12-21T16:44:32.692547Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_replics_df['text'] = train_replics_df['text'].apply(remove_html_tags).apply(remove_nicknames).apply(remove_urls)\neval_replics_df['text'] = eval_replics_df['text'].apply(remove_html_tags).apply(remove_nicknames).apply(remove_urls)\ntest_replics_df['text'] = test_replics_df['text'].apply(remove_html_tags).apply(remove_nicknames).apply(remove_urls)","metadata":{"id":"o1Ge5dccYigZ","execution":{"iopub.status.busy":"2023-12-21T16:44:32.695125Z","iopub.execute_input":"2023-12-21T16:44:32.695489Z","iopub.status.idle":"2023-12-21T16:44:32.788303Z","shell.execute_reply.started":"2023-12-21T16:44:32.695460Z","shell.execute_reply":"2023-12-21T16:44:32.787606Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_replics_df","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"7OeOj_rqpBl8","outputId":"a36b7337-a690-44d8-828a-ad73910bef9c","execution":{"iopub.status.busy":"2023-12-21T16:44:32.789230Z","iopub.execute_input":"2023-12-21T16:44:32.789533Z","iopub.status.idle":"2023-12-21T16:44:32.801694Z","shell.execute_reply.started":"2023-12-21T16:44:32.789508Z","shell.execute_reply":"2023-12-21T16:44:32.800679Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"      dialog_id                                               text  \\\n0             0   for some reason in the blackjacks gun game mo...   \n1             0   It was wired actually. I double checked every...   \n2             0   I’ll try hopping in a game right now and see!...   \n3             0       My last match went flawlessly. Thanks again!   \n4             1  What is going on with the  website .... litera...   \n...         ...                                                ...   \n4223        765     u guys all suck turned my laptop off last n...   \n4224        765     Because of ur shit update so now I'm suppos...   \n4225        765     Oh almost didn't mention I've got auto upda...   \n4226        765   Yeh but it was fine when I turned it off last...   \n4227        765              Oh still hate ms anyway nice 1 geezer   \n\n      is_negative  \n0               0  \n1               0  \n2               0  \n3               0  \n4               1  \n...           ...  \n4223            1  \n4224            1  \n4225            0  \n4226            0  \n4227            1  \n\n[4228 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dialog_id</th>\n      <th>text</th>\n      <th>is_negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>for some reason in the blackjacks gun game mo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>It was wired actually. I double checked every...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>I’ll try hopping in a game right now and see!...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>My last match went flawlessly. Thanks again!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>What is going on with the  website .... litera...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4223</th>\n      <td>765</td>\n      <td>u guys all suck turned my laptop off last n...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4224</th>\n      <td>765</td>\n      <td>Because of ur shit update so now I'm suppos...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4225</th>\n      <td>765</td>\n      <td>Oh almost didn't mention I've got auto upda...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4226</th>\n      <td>765</td>\n      <td>Yeh but it was fine when I turned it off last...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4227</th>\n      <td>765</td>\n      <td>Oh still hate ms anyway nice 1 geezer</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>4228 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Модель","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom transformers import AutoTokenizer\n\ntokenizer_name = 'bert-base-uncased'\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_name)","metadata":{"id":"HECp7bPXFqsL","execution":{"iopub.status.busy":"2023-12-21T17:55:22.441493Z","iopub.execute_input":"2023-12-21T17:55:22.442283Z","iopub.status.idle":"2023-12-21T17:55:22.647714Z","shell.execute_reply.started":"2023-12-21T17:55:22.442250Z","shell.execute_reply":"2023-12-21T17:55:22.646796Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"id":"fQRpfKoVFsKL","execution":{"iopub.status.busy":"2023-12-21T17:55:23.747766Z","iopub.execute_input":"2023-12-21T17:55:23.748544Z","iopub.status.idle":"2023-12-21T17:55:23.753281Z","shell.execute_reply.started":"2023-12-21T17:55:23.748510Z","shell.execute_reply":"2023-12-21T17:55:23.752189Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=2\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137,"referenced_widgets":["665310cd876a49c8a14e90cc4fe3b9a4","eb249a5c925b4070afb3571f43158fd0","e09539267ff04ea0b92b95ee7b779973","15ce900f4e914f6d8016c2e0089591e9","c68e86996b3445aab1d41326b8db8581","f14fc61a79c0434f8ed70fb50f92cc5a","3bb7b69021a741c3a4f9850982e34219","1e7c908a98b947778229075ec157a83d","964dbb4570034e2a858c5fab6686da2b","6ceff80939d245a9ac16a6b5d799d1d4","4af14e169ece4f67b722e7d5cc7573e3","70716fd6a8614cdd9247d151c9e91066","1cdc99e85bc644449b2addb767e79d45","da6d26dfea624a1e91dc0e096f348ec5","3db5cc497fff4907972b7a29d2cb190e","a30d3b2c75624812830039d4b0aef42e","dc4f35404f254513b73eabe7f9d316c6","a8bef80bff3a4df1b78731842a25ec7e","861c47691d41450f8ca6e39bf583f670","d968dddce6e1440db612349f99f71441","c56c6042d4af490a92829c0e5e125300","a581fbf236d9417eac2ce2b77a6505fd"]},"id":"72DMu5PmFsse","outputId":"6f22b7f0-2b51-4237-8d09-e0c89d5a289b","execution":{"iopub.status.busy":"2023-12-21T17:55:23.978556Z","iopub.execute_input":"2023-12-21T17:55:23.978990Z","iopub.status.idle":"2023-12-21T17:55:26.383244Z","shell.execute_reply.started":"2023-12-21T17:55:23.978953Z","shell.execute_reply":"2023-12-21T17:55:26.382457Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4e38916cf5a474d9aa41e7cc64d1139"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ntorch.manual_seed(42)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H1Xopy-0Fue_","outputId":"cdd924e4-b202-4313-ef09-968930ac0639","execution":{"iopub.status.busy":"2023-12-21T17:55:26.384832Z","iopub.execute_input":"2023-12-21T17:55:26.385115Z","iopub.status.idle":"2023-12-21T17:55:26.510592Z","shell.execute_reply.started":"2023-12-21T17:55:26.385090Z","shell.execute_reply":"2023-12-21T17:55:26.509600Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"class ClassificationTorchDataset(Dataset):\n\n    def __init__(self, df):\n        self.df = df\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, idx):\n        model_inputs = tokenizer(\n            self.df.iloc[idx, :]['text'], max_length=512,\n            truncation=True\n        )\n        model_inputs['label'] = self.df.iloc[idx, :][label_name]\n        return model_inputs","metadata":{"id":"_e4NOFRTFwq7","execution":{"iopub.status.busy":"2023-12-21T17:55:26.511894Z","iopub.execute_input":"2023-12-21T17:55:26.512344Z","iopub.status.idle":"2023-12-21T17:55:26.521655Z","shell.execute_reply.started":"2023-12-21T17:55:26.512283Z","shell.execute_reply":"2023-12-21T17:55:26.520351Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"train_replics_dataset = ClassificationTorchDataset(train_replics_df)\neval_replics_dataset = ClassificationTorchDataset(eval_replics_df)","metadata":{"id":"FhXT5ddSFxU-","execution":{"iopub.status.busy":"2023-12-21T17:55:26.524348Z","iopub.execute_input":"2023-12-21T17:55:26.524731Z","iopub.status.idle":"2023-12-21T17:55:26.535517Z","shell.execute_reply.started":"2023-12-21T17:55:26.524694Z","shell.execute_reply":"2023-12-21T17:55:26.534300Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"import evaluate\n\nf1 = evaluate.load(\"f1\")","metadata":{"id":"pfSLO1VfFzGh","execution":{"iopub.status.busy":"2023-12-21T17:55:26.536850Z","iopub.execute_input":"2023-12-21T17:55:26.537214Z","iopub.status.idle":"2023-12-21T17:55:27.115008Z","shell.execute_reply.started":"2023-12-21T17:55:26.537180Z","shell.execute_reply":"2023-12-21T17:55:27.114229Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return f1.compute(predictions=predictions, references=labels)","metadata":{"id":"r9C7W_HVF0Lu","execution":{"iopub.status.busy":"2023-12-21T17:55:27.117042Z","iopub.execute_input":"2023-12-21T17:55:27.117553Z","iopub.status.idle":"2023-12-21T17:55:27.122673Z","shell.execute_reply.started":"2023-12-21T17:55:27.117520Z","shell.execute_reply":"2023-12-21T17:55:27.121763Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"model_dir\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=1e-6,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    num_train_epochs=20,\n    metric_for_best_model='f1',\n    greater_is_better=True,\n    push_to_hub=False,\n    load_best_model_at_end=True\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_replics_dataset,\n    eval_dataset=eval_replics_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":462},"id":"WPAEtBzHF1L0","outputId":"a085d522-f540-4eca-dc5a-6fe9fa4fb0a4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n    with torch.no_grad():\n        logits = model(**inputs).logits\n    return np.array(logits.argmax(axis=1).cpu())","metadata":{"id":"sg_SivkEF4Z9","execution":{"iopub.status.busy":"2023-12-21T17:55:32.506214Z","iopub.status.idle":"2023-12-21T17:55:32.506726Z","shell.execute_reply.started":"2023-12-21T17:55:32.506490Z","shell.execute_reply":"2023-12-21T17:55:32.506509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_replics_df['text']","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S94X-zxyF5Yi","outputId":"cbf5b0bb-57a2-44df-f8f1-da5527ad6172"},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":["0       So neither my iPhone nor my Apple Watch are re...\n","1                                  please read the above.\n","2         My iPhone is on 11.1.2, and my watch is on 4.1.\n","3        I’ve restarted both, also un-paired then re-p...\n","4        Yes, everything seems fine, it’s just Health ...\n","                              ...                        \n","1193     may have been giving out the pre-order copies...\n","1194     but I would just like what I was sold and wha...\n","1195     correctly. I was told by staff in store that ...\n","1196     supply them to me that way? It would be nice ...\n","1197                            what is fair and correct.\n","Name: text, Length: 1198, dtype: object"]},"metadata":{}}]},{"cell_type":"markdown","source":"### Cross-validation:","metadata":{"id":"N9JJalFyviO9"}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"model_dir\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=1e-6,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    num_train_epochs=15,\n    metric_for_best_model='f1',\n    greater_is_better=True,\n    push_to_hub=False,\n    load_best_model_at_end=True,\n    report_to='none'\n)","metadata":{"id":"5aADFR2d04Hk","execution":{"iopub.status.busy":"2023-12-21T17:55:50.324702Z","iopub.execute_input":"2023-12-21T17:55:50.325094Z","iopub.status.idle":"2023-12-21T17:55:50.332519Z","shell.execute_reply.started":"2023-12-21T17:55:50.325063Z","shell.execute_reply":"2023-12-21T17:55:50.331260Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support\nimport evaluate\n\nn = 5\nskf = StratifiedKFold(n_splits=n, random_state=42, shuffle=True)\nf1_score_results = []\n\nfor train_index, test_index in skf.split(df, df[label_name]):\n\n        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n        model.to(device)\n\n        f1 = evaluate.load(\"f1\")\n\n        test_df = df.iloc[test_index]\n        train_df, eval_df = train_test_split(df.iloc[train_index], test_size=0.25, random_state=42)\n\n        train_df = train_df.reset_index(drop=True)\n        eval_df = eval_df.reset_index(drop=True)\n        test_df = test_df.reset_index(drop=True)\n\n        train_replics_df = make_replics_df(train_df, replics_column_name=replics_column_name, label_name=label_name)\n        eval_replics_df = make_replics_df(eval_df, replics_column_name=replics_column_name, label_name=label_name)\n        test_replics_df = make_replics_df(test_df, replics_column_name=replics_column_name, label_name=label_name)\n\n        train_replics_df['text'] = train_replics_df['text'].apply(remove_html_tags).apply(remove_nicknames).apply(remove_urls)\n        eval_replics_df['text'] = eval_replics_df['text'].apply(remove_html_tags).apply(remove_nicknames).apply(remove_urls)\n        test_replics_df['text'] = test_replics_df['text'].apply(remove_html_tags).apply(remove_nicknames).apply(remove_urls)\n\n        train_replics_dataset = ClassificationTorchDataset(train_replics_df)\n        eval_replics_dataset = ClassificationTorchDataset(eval_replics_df)\n\n        trainer = Trainer(\n          model=model,\n          args=training_args,\n          train_dataset=train_replics_dataset,\n          eval_dataset=eval_replics_dataset,\n          tokenizer=tokenizer,\n          data_collator=data_collator,\n          compute_metrics=compute_metrics\n          )\n\n        trainer.train()\n\n        !rm -r model_dir\n\n        labels = test_replics_df[label_name].values.astype('int')\n        predictions = predict(test_replics_df['text'].values.tolist())\n        \n        test_replics_df['prediction'] = predictions\n        \n        dialog_predictions = test_replics_df.groupby('dialog_id')['prediction'].max()\n        dialog_labels = test_replics_df.groupby('dialog_id')[label_name].max()\n\n        precision_only, recall_only, f1_only, support_only = precision_recall_fscore_support(dialog_labels, dialog_predictions)\n\n        f1_score_results.append(f1_only[1])","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"kM_HpA-ivh7F","outputId":"e44a9563-e38b-4992-cade-c54183109d73","execution":{"iopub.status.busy":"2023-12-21T18:39:17.048353Z","iopub.execute_input":"2023-12-21T18:39:17.049462Z","iopub.status.idle":"2023-12-21T19:32:45.819529Z","shell.execute_reply.started":"2023-12-21T18:39:17.049423Z","shell.execute_reply":"2023-12-21T19:32:45.818334Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1680' max='1680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1680/1680 10:35, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.353802</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.322633</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.293494</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.266470</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.337400</td>\n      <td>0.247090</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.337400</td>\n      <td>0.231640</td>\n      <td>0.073529</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.337400</td>\n      <td>0.221410</td>\n      <td>0.314465</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.337400</td>\n      <td>0.214060</td>\n      <td>0.450549</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.224700</td>\n      <td>0.208492</td>\n      <td>0.567164</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.224700</td>\n      <td>0.203900</td>\n      <td>0.596330</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.224700</td>\n      <td>0.201116</td>\n      <td>0.611872</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.224700</td>\n      <td>0.199146</td>\n      <td>0.610619</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.224700</td>\n      <td>0.197348</td>\n      <td>0.620087</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.177700</td>\n      <td>0.196591</td>\n      <td>0.626087</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.177700</td>\n      <td>0.196479</td>\n      <td>0.626087</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1650' max='1650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1650/1650 10:25, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.348653</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.318738</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.290153</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.264617</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.334700</td>\n      <td>0.241955</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.334700</td>\n      <td>0.225488</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.334700</td>\n      <td>0.213947</td>\n      <td>0.106061</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.334700</td>\n      <td>0.205650</td>\n      <td>0.302632</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.334700</td>\n      <td>0.198760</td>\n      <td>0.453488</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.222100</td>\n      <td>0.193557</td>\n      <td>0.554974</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.222100</td>\n      <td>0.190350</td>\n      <td>0.574257</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.222100</td>\n      <td>0.188342</td>\n      <td>0.595122</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.222100</td>\n      <td>0.187398</td>\n      <td>0.598039</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.181900</td>\n      <td>0.186309</td>\n      <td>0.592233</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.181900</td>\n      <td>0.186125</td>\n      <td>0.592233</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1680' max='1680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1680/1680 10:33, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.349551</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.318643</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.288261</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.256674</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.338800</td>\n      <td>0.232686</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.338800</td>\n      <td>0.214976</td>\n      <td>0.088235</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.338800</td>\n      <td>0.202919</td>\n      <td>0.303797</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.338800</td>\n      <td>0.193444</td>\n      <td>0.464088</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.226600</td>\n      <td>0.187169</td>\n      <td>0.517766</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.226600</td>\n      <td>0.182709</td>\n      <td>0.565854</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.226600</td>\n      <td>0.179721</td>\n      <td>0.584906</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.226600</td>\n      <td>0.176273</td>\n      <td>0.611354</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.226600</td>\n      <td>0.174636</td>\n      <td>0.635193</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.183500</td>\n      <td>0.173550</td>\n      <td>0.635193</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.183500</td>\n      <td>0.173200</td>\n      <td>0.638298</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1665' max='1665' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1665/1665 10:26, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.353456</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.321315</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.290974</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.263004</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.327000</td>\n      <td>0.241906</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.327000</td>\n      <td>0.227228</td>\n      <td>0.015625</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.327000</td>\n      <td>0.217420</td>\n      <td>0.331210</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.327000</td>\n      <td>0.209917</td>\n      <td>0.540541</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.327000</td>\n      <td>0.206675</td>\n      <td>0.591837</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.213300</td>\n      <td>0.202446</td>\n      <td>0.599034</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.213300</td>\n      <td>0.199448</td>\n      <td>0.623256</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.213300</td>\n      <td>0.197446</td>\n      <td>0.648649</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.213300</td>\n      <td>0.197509</td>\n      <td>0.642202</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.169300</td>\n      <td>0.196879</td>\n      <td>0.654545</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.169300</td>\n      <td>0.196345</td>\n      <td>0.657658</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/tmp/ipykernel_42/279643666.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df[replics_column_name] = replics_df[replics_column_name].\\\n/tmp/ipykernel_42/279643666.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  replics_df['text'] = replics_df['text'].map(get_client_replics_list)\n/tmp/ipykernel_42/279643666.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  bad_indices = neg_replics[neg_replics.groupby('index')['is_bad_replic'].sum() > 0]['index'].unique()\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1695' max='1695' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1695/1695 10:31, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.381377</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.351461</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.319986</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.286078</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.329800</td>\n      <td>0.261255</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.329800</td>\n      <td>0.244315</td>\n      <td>0.039735</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.329800</td>\n      <td>0.233733</td>\n      <td>0.275862</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.329800</td>\n      <td>0.223841</td>\n      <td>0.424242</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.214400</td>\n      <td>0.222294</td>\n      <td>0.488038</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.214400</td>\n      <td>0.216024</td>\n      <td>0.566372</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.214400</td>\n      <td>0.211803</td>\n      <td>0.607595</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.214400</td>\n      <td>0.210358</td>\n      <td>0.634146</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.214400</td>\n      <td>0.207349</td>\n      <td>0.642857</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.169300</td>\n      <td>0.206695</td>\n      <td>0.637795</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.169300</td>\n      <td>0.206888</td>\n      <td>0.640316</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -r model_dir","metadata":{"execution":{"iopub.status.busy":"2023-12-21T18:22:40.540681Z","iopub.execute_input":"2023-12-21T18:22:40.541030Z","iopub.status.idle":"2023-12-21T18:22:43.605932Z","shell.execute_reply.started":"2023-12-21T18:22:40.541004Z","shell.execute_reply":"2023-12-21T18:22:43.604657Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"f1_score_results","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"69bf28b1-7b23-48c9-aca0-d547c67b9eb7","id":"WLrWILZSxbIH","execution":{"iopub.status.busy":"2023-12-21T19:33:26.519330Z","iopub.execute_input":"2023-12-21T19:33:26.520533Z","iopub.status.idle":"2023-12-21T19:33:26.528842Z","shell.execute_reply.started":"2023-12-21T19:33:26.520388Z","shell.execute_reply":"2023-12-21T19:33:26.527734Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"[0.676923076923077,\n 0.7559055118110236,\n 0.7299270072992701,\n 0.7647058823529411,\n 0.6865671641791046]"},"metadata":{}}]},{"cell_type":"markdown","source":"### Итоговый результат","metadata":{}},{"cell_type":"code","source":"sum(f1_score_results) / len(f1_score_results)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s55YEnkgAvoS","outputId":"4df12e70-a5ce-41ac-c151-938e8d1ae2d2","execution":{"iopub.status.busy":"2023-12-21T19:33:27.975581Z","iopub.execute_input":"2023-12-21T19:33:27.975947Z","iopub.status.idle":"2023-12-21T19:33:27.982508Z","shell.execute_reply.started":"2023-12-21T19:33:27.975914Z","shell.execute_reply":"2023-12-21T19:33:27.981547Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"0.7228057285130833"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"ex9TQkvH8kEP"},"execution_count":null,"outputs":[]}]}